{
 "metadata": {
  "name": "",
  "signature": "sha256:2103a4a31fb72fbe741c8af2e3f6942ff9ec8d27f978b06c34fd5c029ec31c95"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Insert punny title here"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Overview and Motivation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Insert description here*\n",
      "This [title of report or article or dataset] contains information from the Harvard Library Bibliographic Dataset, which is provided by the Harvard Library under its Bibliographic Dataset Use Terms and includes data made available by, among others, OCLC Online Computer Library Center, Inc. and the Library of Congress."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Related Work"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*More description*"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Initial Questions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our initial question with this dataset is whether we can predict the number of checkouts for a library item based on its attributes (e.g. year of publication, subject matter, holding library, etc.)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Data Scraping"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import all necessary modules\n",
      "import sys\n",
      "import pandas as pd\n",
      "import json\n",
      "import requests\n",
      "from urllib2 import Request, urlopen\n",
      "from pandas.io.json import json_normalize\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our data comes from the Harvard Library Open Metadata. A description of this data can be found at http://openmetadata.lib.harvard.edu. Briefly, this dataset contains over 12 million records of the items in Harvard's libraries. Since our question had to do with the number of checkouts for each item, we only took the subset of items which had at least one checkout by an undergraduate student. \n",
      "\n",
      "For the purpose of data scraping, we are using the Harvard LibraryCloud Item API (http://librarycloud.harvard.edu/v1/docs/item/). Since the number of items returned is capped at 250 per request, we wrote a few functions to help us grab all the data of interest. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a list of valid search fields\n",
      "valid_fields = []"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def num_records(field, query):\n",
      "    \"\"\"\n",
      "    Returns the number of records contained in \n",
      "    Harvard Library Metdata matching the given\n",
      "    field and query\n",
      "    \"\"\"\n",
      "    url = 'http://librarycloud.harvard.edu/v1/api/item/?filter='+str(field)+':'+str(query)\n",
      "    r = requests.get(url)\n",
      "    info = r.json()\n",
      "    # num_records is the total number of records returns for the given field and query\n",
      "    records = info['num_found']\n",
      "    return records"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_urls(field, query):\n",
      "    \"\"\"\n",
      "    Returns a list of URLs which contain all \n",
      "    records of the given field and query\n",
      "    \"\"\"\n",
      "    records = num_records(field, query)\n",
      "    if records == 0:\n",
      "        return\n",
      "    # Create the base string for url that is to be modified and added to list\n",
      "    base_url = 'http://librarycloud.harvard.edu/v1/api/item/?filter='+str(field)+':'+str(query)+'&limit=250'\n",
      "    num_loops = records/250 + 1\n",
      "    urls_list = []\n",
      "    for i in xrange(num_loops):\n",
      "        loop_url = base_url + '&start=' + str(i*250)\n",
      "        urls_list.append(loop_url)\n",
      "    return urls_list\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scrape_records(url):\n",
      "    \"\"\"\n",
      "    Reads the JSON object at the given url and \n",
      "    returns the data as a Pandas dataframe\n",
      "    \"\"\"\n",
      "    request = Request(url)\n",
      "    cols = list()\n",
      "    response = urlopen(request)\n",
      "    books = response.read()\n",
      "    data = json.loads(books)\n",
      "    df = json_normalize(data['docs'])\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Find the maximum number of checkouts to use for scraping"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Check for a maximum of undergraduate checkouts to set parameter for scraping\n",
      "checkout_min = 3800\n",
      "checkout_max = 7000\n",
      "# Preallocate memory for each entry in the list\n",
      "checkout_list = [None]*(checkout_max-checkout_min)\n",
      "for k in xrange(checkout_min, checkout_max):\n",
      "    num_checkout = num_records('score_checkouts_undergrad', k)\n",
      "    checkout_list[k-checkout_min] = k, num_checkout\n",
      "df = pd.DataFrame(checkout_list)\n",
      "# Print the maximum checkout number with at least 1 item returned by num_records\n",
      "max(df.ix[df[1]>0][0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 71,
       "text": [
        "3868"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One record has 3868 checkouts by undergraduates. Let's see what it is!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = create_urls('score_checkouts_undergrad', 3868)\n",
      "b = [scrape_records(a[j])for j in xrange(len(a)) ]\n",
      "b[0].ix[0].title"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 78,
       "text": [
        "u'Physics for scientists & engineers with modern physics'"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Turns out it's *Physics for scientists and engineers with modern physics*. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Get a master list of URLs to scrape using the API"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can make our master list of URLs from which we'll use the API to get the information into a pandas dataframe."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create master list of URLs - ONLY RUN ONCE! Next time can load from the file below\n",
      "master_urls = [create_urls('score_checkouts_undergrad', i) for i in xrange(1, 3870)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Save the master url list\n",
      "f = open('url_list', 'w')\n",
      "json.dump(master_urls, f)\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load the master url list\n",
      "f2 = open('url_list', 'r')\n",
      "test = json.load(f2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get rid of None values in master_urls\n",
      "master_urls_filtered = filter(None, master_urls)\n",
      "# Flatten the list of lists\n",
      "flat_master_urls = [url for url_list in master_urls_filtered for url in url_list]\n",
      "# Save the flattened, filtered list\n",
      "f3 = open('filtered_url_list', 'w')\n",
      "json.dump(flat_master_urls, f3)\n",
      "f3.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 107
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Get all the data into a Pandas Data Frame"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "START HERE for scraping data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load the master list of urls\n",
      "f4 = open('filtered_url_list', 'r')\n",
      "flat_master_urls = json.load(f4)\n",
      "f4.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use a subset of the URLs to get a sense of how long scraping will take\n",
      "test_urls = flat_master_urls[20:30]\n",
      "# Use timeit in python to time the speed of the process\n",
      "start = time.time()\n",
      "test_df_list = [scrape_records(url) for url in test_urls]\n",
      "end = time.time()\n",
      "print 'It took %f seconds to scrape 10 urls' %(end-start)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "It took 15.608278 seconds to scrape 10 urls\n"
       ]
      }
     ],
     "prompt_number": 122
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check how many URLs we have to go through"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(flat_master_urls)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "2802"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_df = pd.concat(test_df_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 124
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_df.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 128,
       "text": [
        "Index([u'call_num', u'creator', u'data_source', u'dataset_tag', u'format', u'height', u'height_numeric', u'holding_libs', u'id', u'id_inst', u'id_isbn', u'id_lccn', u'id_oclc', u'language', u'lcsh', u'loc_call_num_sort_order', u'loc_call_num_subject', u'note', u'online_avail', u'pages', u'pages_numeric', u'pub_date', u'pub_date_numeric', u'pub_location', u'publisher', u'score_checkouts_fac', u'score_checkouts_grad', u'score_checkouts_undergrad', u'score_course_texts', u'score_downloads', u'score_holding_libs', u'score_recalls', u'score_reserves', u'shelfrank', u'source_record.010a', u'source_record.010z', u'source_record.012a', u'source_record.012b', u'source_record.012l', u'source_record.0152', u'source_record.015a', u'source_record.0162', u'source_record.016a', u'source_record.020a', u'source_record.020b', u'source_record.020c', u'source_record.020z', u'source_record.0222', u'source_record.022a', u'source_record.022l', u'source_record.022y', u'source_record.022z', u'source_record.024a', u'source_record.024c', u'source_record.024d', u'source_record.025a', u'source_record.028a', u'source_record.028b', u'source_record.029a', u'source_record.029b', u'source_record.030a', u'source_record.032a', u'source_record.032b', u'source_record.033a', u'source_record.033b', u'source_record.034a', u'source_record.035a', u'source_record.035z', u'source_record.037a', u'source_record.037b', u'source_record.037c', u'source_record.037n', u'source_record.040a', u'source_record.040b', u'source_record.040c', u'source_record.040d', u'source_record.041a', u'source_record.041b', u'source_record.041d', u'source_record.041e', u'source_record.041g', u'source_record.041h', u'source_record.041j', u'source_record.042a', u'source_record.043a', u'source_record.044a', u'source_record.045a', u'source_record.045b', u'source_record.049a', u'source_record.049o', u'source_record.050a', u'source_record.050b', u'source_record.055a', u'source_record.055b', u'source_record.060a', u'source_record.060b', u'source_record.070a', u'source_record.070b', u'source_record.0722', u'source_record.072a', ...], dtype='object')"
       ]
      }
     ],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}